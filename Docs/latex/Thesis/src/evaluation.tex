The evaluation of a single submission to the challenge is related to the previously mentioned \emph{significance level}.
We define the \emph{significance}

\begin{equation}\label{eq:Z}
	Z = \sqrt{2 \left(n \ln{\left( \frac{n}{\mu_b} \right)} -
	n + \mu_b \right)}
\end{equation}

, where $n$ is the total number of observed events and bigger than $\mu_b$, which is the expected number of background-events.

In other words, we investigate a region in feature space and expect a number of events ($\mu_b$), $n$ is the number we \emph{actually} observe in this region. In particle physics, a significance of at least $Z=5$, called a \emph{five-sigma effect}, is regarded as sufficient to claim a discovery \cite{higgsPaper}. This is equivalent to the previously stated \emph{significance level} $p = 2.87 \times 10^{-7}$.

We can substitute $n$ with $s+b$ with $\mu_b$ for $b$ and transform Eq. \eqref{eq:Z} to the \emph{Approximate Median Significance} (AMS)

\begin{equation}\label{eq:AMS_'}
	AMS' = \sqrt{2 \left( \left( s+b \right) \ln{ \left(1+ \frac{s}
	{b}  \right)} - s \right)}
\end{equation}

, which resembles an estimation function that is used by high-energy physicists for optimizing the selection region for stronger discovery significance \cite{higgsPaper}. 

For the challenge, a regularization-term $b_{reg}$ was introduced as an artificial shift to $b$ to decrease variance of the AMS, as this makes it easier to compare the participants if the optimal signal region was small. The challenges documentation provides no further information on the choice $b_{reg}=10$ \cite{higgsPaper}.

This addition to Eq. \eqref{eq:AMS_'} makes the final evaluation-formula complete:

\begin{equation}\label{eq:AMS}
	AMS = \sqrt{2 \left( \left( s+b+b_{reg} \right) \ln{ \left(1+ \frac{s}
	{b+b_{reg}}  \right)} - s \right)}
\end{equation}

For simplicity, we will call it just AMS, as Eq. \eqref{eq:AMS_'} will not have further appearances in this Thesis.

\subsubsection{AMS as Objective Function in Classification}
In terms of data classification, an \emph{objective function} is a tool to estimate the accuracy of a fitted classifier. An effective approach to optimize the training of a classification algorithm is to optimize the used objective function. In other words, we derive and minimize it.

If we partially derive Eq. \eqref{eq:AMS} with respect to $s$, we get

\begin{equation}\label{eq:AMS_der}
	\frac{\partial}{\partial s} AMS = 
	\frac{\frac{s + b + b_{reg}}
	{ ( b + b_{reg} ) \left( \frac{s}{b + b_reg} + 1 \right)}
	+ \ln \left( \frac{s}{b + b_reg} + 1 \right) - 1}
	{ \sqrt{2 \left( ( s + b + b_{reg} ) \ln{ \left( 1 +  \frac{s}{b+b_{reg}} \right)} - s \right)}}
\end{equation}

The complexity of Eq. \eqref{eq:AMS_der} is the reason why most participants of the challenge avoided AMS as a primary objective function. After the actual prediction, which is made by the classifier fitted to another objective, the events are ranked according to their likelihood of being a signal. The AMS can now be used to determine an optimal decision threshold for the final classification. For most submissions, given their ranking works correctly, classifying the top 14\% of most likely events as signal resulted in optimal AMS \cite{cowa14}. \\
Many participants simply substituted with common objective functions like \emph{squared error} that are computational efficient. Some investigated AMS further and found related functions with beneficial properties, like easier derivation \cite{kotl14,mack14,diaz14}.