
\section{Methods of classification}\label{ch:methods}
There are countless possible ways in data science to classify data. The efficiency of methods can vary in wide ranges for different types of problems, possibly taking a great amount of time while having a poor prediction accuracy.

In Sect. \ref{sec:basic} we learn about methods that are commonly used by data scientists. Following this, we discuss criteria for the choice of classifiers for an own approach in Sect. \ref{sec:choice}. Based on these circumstances we present \emph{Logistic Regression} in Sect. \ref{sec:logReg} and \emph{k Nearest Neighbors} in Sect. \ref{sec:kNN} as own approaches, followed by the winning submission by Gábor Melis in Sect. \ref{sec:win}. The chapter ends with Sect. \ref{sec:xgb}, presenting \emph{XGBoost} as winner of the \emph{HEP meets ML Award}.

\subsection{Basic data science methodology}\label{sec:basic}
Before we learn about criteria for choosing a good classification method, we present basic methods in data science. The performance of our own approach can be influenced by the right use of these procedures.

\subsubsection{Feature selection and engineering}
An intuitive approach to optimize classifiers is to vary the used features of the data sets. The selection can be performed manually by considering e.g. histograms (Fig. \ref{fig:hist1}) , scatter plots (Fig. \ref{fig:scat1}), performance of various classifiers or automatically using techniques like PCA. Many automatic procedures tend to exclude features based on max errors, while the challenge aims to maximize AMS. Using such methods risks loosing important features\cite{blog}.

In the data, the value \emph{-999.0} is often set for a number of features. This is actually a flag for \emph{missing} measurement in an event as sometimes a property simply cannot be measured, because fundamental effects do not happen.
The defining feature for missing values is \texttt{PRI\_jet\_num}, which is equal to the number of jets measured in a \emph{pp} collision. Given an event, where \texttt{PRI\_jet\_num} equals \emph{0}, there are 10 features that do not exist, only missing values in \texttt{DER\_mass\_MMC} are not related to this feature. The whole original dataset contains 567329 events with at least one feature missing, excluding missing \texttt{DER\_mass\_MMC}. This equals 70.9\% of all events and has to be considered for some classifiers. We bypass this problem by cutting these features for most methods. For simplicity, we only use the feature subsets presented in Tab. \ref{tab:feats} and the complete set of the original data.

\begin{table}
\begin{center}
\begin{minipage}{\textwidth}
	\includegraphics[width=\linewidth]{images/featsets}
	\vspace*{\baselineskip}
\end{minipage}
\begin{minipage}{.6\textwidth}
	\includegraphics[width=\linewidth]{images/featsets-legend}
\end{minipage}
\hspace{1em}
\begin{minipage}{.35\textwidth}
	\caption{\\Feature subsets used by our own approaches}
	\label{tab:feats}	
\end{minipage}
\end{center}
\end{table}

These sets are created before and during classification, latter as part of optimization effort. Sets 1 and 2 are chosen beforehand as result of the visualization of the features, e.g. \texttt{DER\_mass\_MMC} seems to be good separable, so it is added to most sets although it has missing values. Sets 3,4 and 5 result of an ensemble of 50 \emph{k Nearest Neighbor} classifiers, which will be described in Sect. \ref{sec:kNN}, each choosing 5 random features. These sets contain a number of the common features used for the best predictions generated by this ensemble. The Sets 6 and 7 are chosen by hand as improvement affords. Set 8 results of an ensemble of \emph{Logistic Regression} classifiers (Sect. \ref{sec:logReg}).Like before, we choose thest common features used for the best predictions of the ensemble. Set 9 is the only feature set suggested by a competitor in the challenge \cite{blog}.

Because of its origin in physics, the calculation of more topic-relevant features seems as a promising use of domain-knowledge. However, the top-participants of the challenge shared the observation that artificial features did not improve the AMS significantly \cite{melis-1st,salimans-2nd,courtiol-3rd}.

\subsubsection{Cross-validation}
Overfitting a model is a known problem in machine learning. In the leaderboards we can observe single submissions to the challenge that had big differences in ranks on the public and private leaderboard. This could be explained by overfitted classifiers, in one case a submission fell 720 ranks from its public to private rank. \\
A common technique to prevent overfitting is to use only one part of the training set for fitting the classifier while using the other one for testing, calculating a evaluation score, like \emph{mean squared error}(MSE). This method is called \emph{cross-validation}.  Multiple contestants stated, that simple cross-validation is not able to reliably estimate a submissions AMS. A solution to this is to repeat this procedure with different splits of the training data, which heavily increases the runtime of classifiers that rely on estimation by cross-validation.

\subsection{Choosing the classifiers}\label{sec:choice}
In order to choose an effective approach to a solution for the challenges task, we need to consider several dependencies of a classifier.

\subsubsection{Computational resources}
One basic limitation of our choice are the available resources.
We want our classification to efficiently use the system represented by Tab. \ref{tab:specs}.

\begin{SCtable}[1][h]
  	\begin{tabular}{r l||}
		Operating System: & Windows 8.1 64bit\\
		CPU: & Intel i7 K875 @ 2.93GHz (4 Cores)\\
		Memory: & 16GB DDR3\\
		GPU: & NVIDIA GeForce GTX470 @ 1.22GHz\\
		VRAM: & 1280 MB GDDR5\\
	\end{tabular}
	\caption{\\Specifications of used System}
	\label{tab:specs}
\end{SCtable}

While this hardware enables us to use various methods that utilize minor  parallelization, it still limits the performance of classifiers dependent on high parallelization, like Neural Networks. If not stated differently, all presented data on \emph{runtime} in this Thesis is based on the system represented by Tab. \ref{tab:specs}.

\subsubsection{Dependency on data}
The choice of classifiers is dependent on the amount of data. The training set contains 250000 samples with 32 features (EventId not counted), the test set 550000 samples with 30 features. Considering this, some classification methods would be ineffective, e.g. \emph{Support Vector Classification} due to its complexity $>O(n^2)$\cite{sklearn}.

\subsubsection{scikit-learn}
For all own approaches to the challenge we will use version 0.17 of \emph{scikit-learn}, an open source machine learning library for Python. Originated from a "Google Summer of Code"-project started 2007, it grew into a popular toolkit \cite{sklearn}.
Using scikit-learn has advantages as disadvantages, Tab. \ref{tab:sklearn} considers the most important ones.

\begin{table}[h]
\begin{center}
	\begin{tabular}{ | p{0.3\textwidth} | p{0.65\textwidth} |}
  		\hline
		\multicolumn{2}{ |c| }{Advantages} \\
		\hline
		Usability &
		Scikit-learn is easily installed via Python-package manager systems like \emph{pip} or \emph{Anaconda}, its required dependencies are NumPy, SciPy, and a working C/C++ compiler\cite{sklearngit}. It is compatible with Windows 8.1 64bit.\\
		\hline
		Scale and documentation &
		A reason for its popularity is the amount of common algorithms this toolkit contains. It also offers good code-documentation and mathematical explanation of all models included.\\
		\hline
		Multi-core CPU support &
		With every version, scikit-learn improves its multi-core support for algorithms that profit of parallelization\cite{sklearnhistory}. This will speed up our classification considerably.\\
		\hline
		\hline
		\multicolumn{2}{ |c| }{Disadvantages} \\
		\hline
		No GPU support: &
		While the library utilizes multi-core CPUs, GPU-parallelization is not supported by scikit learn.\\
		\hline
		(Almost) no custom \newline scoring functions &
		Scikit-learn offers rarely an easy way to use custom evaluation functions for e.g. fitting a classifier. Some algorithms, like \emph{sklearn.cross\_validation}, offer choice of pre-built functions via a \emph{scoring}-parameter.\\
		\hline
		Speed &
		One focus of the development of the toolkit is optimizing and speed-up of the algorithms, but projects devoted to single learning techniques are probably faster.\\
		\hline
	\end{tabular}
	\caption{Comparison of scikit-learns properties}
	\label{tab:sklearn}
	\end{center}
\end{table}

Sec. \ref{sec:logReg} and \ref{sec:kNN} describe our approach, the source code is available at \url{https://github.com/gargi/BA_git}.

\input{logReg}

\input{knn}

\subsection{The winning submission}\label{sec:win}
The three top submissions of the challenge all use classification methods that rely heavily on strong hardware. Without the ability to recreate these submissions or use various parameter-changes, we focus solely on understanding the winning model, relying on \cite{meli14}.

\subsubsection{Neural Networks in general}
A \emph{Neural Network} is a type of machine learning models that was inspired by the function of central nervous systems, like the human brain. Blocks or \emph{neurons} are basic mathematical operations, like simple linear or bias functions. These neurons are grouped into so-called \emph{layers}, which can be interpreted as weighting-function. The weights are optimized via loss functions, like \emph{Squared Error}, resembling the training phase of this learning model. Usually Neural Networks have an \emph{input layer}, a number of \emph{hidden layers} and an \emph{output layer}. An user, or an external algorithm, can only interact through the input layer, delivering data to the network, and the output layer, receiving an estimation on this data.
In general the layers of a Neural Networks can be expressed mathematically (\cite{harmeling18}).

%\begin{table}[h]
\begin{center}
	\begin{tabular}{r l l}
		& $z_1 = x$ & input layer\\
		& $z_2 = f_1(w_1,z_1)$ & second layer (hidden)\\
		& $z_3 = f_1(w_2,z_2)$ & third layer (hidden)\\
		$y = $  & $z_4 = f_1(w_3,z_3)$ & output layer\\
	\end{tabular}
\end{center}
	%\caption{Schematic representation of a Neural Network\cite{harmeling18}}
	%\label{tab:layers}
%\end{table}

\subsubsection{The method}
The model developed by Gábor Melis uses an ensemble of 70 Neural Networks with three hidden layers containing 600 neurons each. The output layer consists of two \emph{softmax} units, which basically normalize the weight-combinations to get a prediction for each class.
Each Neural Network is fitted by a different split of the training-data that is normalized to mean 0 and variance 1, and different weight-initializations. The hidden layers utilize \emph{Local Winner-Take-All activation}, which cancel all weighted input values received by three blocks except for the maximum. The weighting is optimized with respect to a cross-entropy objective function using backpropagation with stochastic gradient descent.
The winning submission, which has been generated by this classifier, outperforms follow-up submissions by over 0.015 AMS. It uses artificial features and was created within a runtime of 10 minutes on a GTX Titan GPU \cite{meli14}. However, the difference to the second place only equals under 0.4\% of the Neural Networks private score.

\input{xgboost}

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Leerseite bei zweiseitigem Druck
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifthenelse{ \( \equal{\zweiseitig}{twoside} \and \not \isodd{\value{page}} \)}
	{\pagebreak \thispagestyle{empty} \cleardoublepage}{\clearpage}