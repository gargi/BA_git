\section{Introduction}\label{ch:intro}\raggedbottom
With the beginning of this work, I will present the history of the ATLAS experiment and data processing at CERN, which enabled the discovery of the Higgs Boson in 2012. This is followed by a summary about discoveries with the use of statistics, which is a common technique in modern experimental particle physics.
After a brief description of the platform Kaggle, I establish a connection by explaining the Higgs Boson Machine Learning Challenge and its goals. It is concluded by an overview of the thesis structure.

\subsection{The Higgs Boson and the ATLAS experiment}
In 1964, three teams of physicists predicted the existence of a sub-atomic particle to give mass to other elementary particles. This so-called \emph{Higgs Boson} became a key-requirement of the \emph{Standard Model of particle physics}, a collection of theories that successfully predicted many other particles.

The European Organization for Nuclear Research (CERN\footnote{CERN is the abbreviation of \emph{Conseil Europ\`een pour la Recherche Nucl\`eaire}, a council founded in 1952, assigned to establish a European research organization \cite{cernHP}.}) approved the construction of the \emph{Large Hadron Collider} (LHC) in 1994. This ring-formed machine with a  27-km circumference has been designed to involve experiments with the purpose to investigate some of the most-important questions in particle physics. Finding evidence to confirm or annul the existence of the Higgs Boson was a central purpose of the experiments \emph{ATLAS} and \emph{CMS}.

After CERN announced July 2012 its discovery, Peter Higgs and Fran\c ois Englert were acknowledged with the Nobel Price in Physics 2013 for predicting the Higgs Boson.

\subsubsection{Data processing of ATLAS}
With the configuration used during its first runtime from 2009 to 2013, the LHC produces \emph{events} at a rate up to 40 million per second (40MHz), each event containing at least one \emph{Proton-Proton} (\emph{pp}) -interaction and hundreds of new particles, summarized in a raw-data vector of about a hundred thousand dimensions \cite{higgsPaper}. This data is reduced via three levels of hard- and software filters, so-called \emph{triggers}, discarding all but the most interesting data. This process scales the output-data of ATLAS down to about 400\footnote{Used sources are not consistent about the amount of events \cite{higgsPaper, glig14, atlasHP}.} compressed events per second, adding up to 320 Mbyte of data per second, which is recorded. This raw data is reconstructed by ATLAS to full-events in several stages. First, easy-calculable information is derived to be additionally used for classification, called \emph{event selection}.\\
This selection estimates the probability for an event to be significant for the current task, like finding the Higgs Boson. If the probability is high enough, the event enters the next reconstruction-stage to calculate other previously cut features that need more computation time and is recorded as part of the  so-called \emph{selection region}. The other events are referred as \emph{background} and are not processed further for the current task.\\
These stages of reconstruction work time-budgeted and only calculate needed features of an event\footnote{The reconstruction of a full event was planned to cost 15 seconds, though this estimation relies on technical standards of 2008 \cite{atlasHP}.}.
ATLAS extracts physics information from reconstructed events, containing only 0.1 Mbytes of data. This data describes actual physical objects, like particles and their trajectory.

\subsubsection{Claiming a discovery}
In particle physics, the choice of the selection region is key for improving the \emph{statistical significance} of a discovery. Given an observation that is assumed to be caused by a specified effect. This relation, the discovery, is considered \emph{significant} if the chance of it being caused by any other effect is less than the \emph{significance level}, a previously determined value. For the successful discovery of the Higgs Boson, this value was $p = 2.87 \times 10^{-7}$ \cite{higgsPaper}. We can conclude that \emph{improving statistical significance} is equivalent to reducing the chance of the observation being caused by background-events.

After the discovery in 2012, CERN started to strengthen the evidence for the Higgs Boson by proving various properties that a particle has to fulfil to be the predicted object of 1964. One property is called \emph{Higgs to tau tau decay} ($\mathrm{H}\rightarrow\tau\tau $) and was observed in 2013 \cite{atlasNote}.

\subsection{The Higgs Boson Machine Learning Challenge}
In May 2014 CERN announced a challenge to be hosted on the data science platform Kaggle. Counting 1785 participating teams it was the most popular challenge on the website till early 2015.

\subsubsection{Kaggle}
Kaggle is an internet platform for data scientists, founded 2010. Besides hosting challenges, its services include hosting public datasets, a job market for data scientists and "Kaggle Rankings", a scoreboard based on performances of community-members in Kaggles competitions. All these services are free to an individual member, but paid by businesses, e.g. for hosting a challenge. Further, Kaggle works as consultant in structure or preparation for a challenge \cite{kaggle}.

\subsubsection{The leaderboard}\label{sec:lb}
In Kaggle-Challenges, competitors are ranked in a leaderboard, rank 1 is the participant who submitted a solution which achieved the best score on the evaluation used in this challenge, in our case the \emph{Approximate Median Significance} (AMS), which is derived in Chap. \ref{ch:challenge}. To prevent participants from simply training algorithms by directly optimizing the leaderboard-score, Kaggle uses so-called \emph{public} and \emph{private leaderboards}, which use different data subsets from the whole test set to generate the AMS. For every Kaggle challenge, the data is split by 30\% for the public and 70\% for the private leaderboard.

During the challenge, the public leaderboard is visible to any visitor of Kaggle, so participants are able to get an evaluation of their submitted solution and work on better classification for a higher rank.
The private leaderboard, which shows the final ranking of the challenge and the differences to the public leaderboard, is accessible after reaching the final submission deadline. Only the private rank is relevant for winning a Kaggle competition. In Chap. \ref{ch:disc} we discuss the differences in this challenges ranking. For now we conclude to use the public AMS to evaluate our own classification-approaches as this might enable better insight to what might cause the differences.

For comparison, the competition hosts added various benchmarks to the leaderboard [Tab.\ref{tab:benchmarks}]. A tuned submission  achieved private rank 782 by using \emph{Toolkit for Multivariate Data Analysis with ROOT} (TMVA), a widely used software used in High Energy Physics \cite{cowa14}.

\begin{table}
	\begin{center}
		\begin{tabular}{ | l | c | c | }
		    \hline
		    Benchmark & public AMS & private AMS \\
		    \hline
	    	random submission & 0.58477 & 0.58648 \\
		    \hline
		    simple window & 1.54451 & 1.53518\\
		    \hline
		    naive Bayes starting kit & 2.06036 & 2.06021 \\
		    \hline
		    simple TMVA boosted trees & 3.24954 &  3.19956 \\
		    \hline
		    multiboost & 3.34085 &  3.40488 \\
		    \hline
		\end{tabular}
		\caption{Benchmarks provided by CERN}
		\label{tab:benchmarks}
	\end{center}
\end{table}

\subsubsection{Goals}
While the task of \emph{The Higgs Boson Machine Learning Challenge} is to strengthen the observation of $\mathrm{H}\rightarrow\tau\tau $ by training a classifier for event selection, another important goal of the challenge is to promote the collaboration of data science and physics. Over the next years, CERN expects hardware-improvements that make much higher rates in data processing possible. However, to utilize the hardware, a considerable improvement of algorithm speed is required \cite{glig14}.
Beside the three prizes going to the top-three submissions, a special jury award has been offered. The team that delivers an efficient classification model in terms of accuracy, simplicity and performance requirements is acknowledged with an invitation to meet the ATLAS team at CERN.
This price marks the important properties of algorithms that will be used to overcome the increasing amount of LHC data in the future.

\subsection{Overview}
In Chap. \ref{ch:challenge}, we will describe the structure of the challenges dataset \cite{higgsData} and use simple data-analysis methods, to gain first insight about useful features.
After we derive the formal problem from the challenges task, we conclude this chapter by understanding the evaluation metric AMS as objective functions for optimizing our classifiers. 

At its beginning, Chap. \ref{ch:methods} presents basic data science methodology and follows with several dependencies we need to consider in the choice of own simple simple approaches for classification in Sec. \ref{sec:logReg} and \ref{sec:kNN}. After these we will describe the more specific and complex methods \emph{Neural Networks} and \emph{Gradient Boosting Classification}.

In Chap. \ref{ch:disc} we discuss the performance of the methods presented beforehand on the challenges data. We consider the impact of \emph{The Higgs Boson Machine Learning Challenge} on Kaggle and on particle physics.

The Thesis is concluded by Chap. \ref{ch:conc}. We recap the whole work and review key competences for succeeding on Kaggle.
This is closed by a short personal view.
\pagebreak
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Leerseite bei zweiseitigem Druck
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifthenelse{ \( \equal{\zweiseitig}{twoside} \and \not \isodd{\value{page}} \)}
	{\pagebreak \thispagestyle{empty} \cleardoublepage}{\clearpage}