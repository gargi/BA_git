{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%Date: 04.01.2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using logistic Regression on Toydata to get a high AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import linear_model as linMod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data shall have the form of $[w,y,x_1,x_2]$ where\n",
    "\n",
    "- $w$ is a weight in the intervall $[0,1)$\n",
    "- $y$ is the label \"0\" for \"background\" or \"1\" for \"signal\"\n",
    "- $x_n$ are randomly generated features with respect to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generateFeature(label, mu_s, mu_b, sigma_s=5, sigma_b=5):\n",
    "    if label is 1:\n",
    "        mu = mu_s\n",
    "        sigma = sigma_s\n",
    "    else:\n",
    "        mu = mu_b\n",
    "        sigma = sigma_b\n",
    "    return np.random.normal(mu,sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate Median Significance (AMS) defined as:\n",
    "\n",
    "$$AMS = \\sqrt{2 { (s + b + b_r) log[1 + (s/(b+b_{reg}))] - s}}$$     \n",
    "        \n",
    "where \n",
    "\n",
    "- $b_{reg} = 10$ is a regulization term (set by the contest),\n",
    "- $b = \\sum_{i=1}^{n} w_i, y_i=0$ is sum of weighted background (incorrectly classified as signal),\n",
    "- $s = \\sum_{i=1}^{n} w_i, y_i=1$ is sum of weighted signals (correctly classified as signal),\n",
    "- $log$ is natural logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calcAMS(s,b):    \n",
    "    br = 10.0\n",
    "    radicand = 2 *( (s+b+br) * math.log (1.0 + s/(b+br)) -s)\n",
    "    if radicand < 0:\n",
    "        print('radicand is negative. Exiting')\n",
    "        exit()\n",
    "    else:\n",
    "        return math.sqrt(radicand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def calcWeightSums(weights,preds,labels):\n",
    "    s = 0\n",
    "    b = 0\n",
    "    for j in list(range(0,len(preds))):\n",
    "        pred = preds[j]\n",
    "        label = labels[j]\n",
    "        weight = weights[j]\n",
    "        if pred > 0.:\n",
    "            if label > 0.:\n",
    "                s += weight\n",
    "            else:\n",
    "                b += weight\n",
    "    return s,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#toydata shall have n vectors with 5 dimensions\n",
    "n = 100000\n",
    "#probability for signal-label\n",
    "s_prob = 0.05\n",
    "#random values will be used as weights for evaluation later\n",
    "weights = np.random.rand(n)\n",
    "labels = np.zeros(n)\n",
    "x_1 = np.zeros(n)\n",
    "x_2 = np.zeros(n)\n",
    "\n",
    "for i in range(0,n):\n",
    "    if weights[i] <= s_prob:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    labels[i] = label\n",
    "    x_1[i]=generateFeature(label,mu_s=5,mu_b=20)\n",
    "    x_2[i]=generateFeature(label,mu_s=5,mu_b=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%pylab inline\n",
    "plt.scatter(x_1, x_2, edgecolor=\"\", c=labels, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def splitList(xList,n):\n",
    "    aList = xList[:n]\n",
    "    bList = xList[n:]\n",
    "    return aList,bList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split toydata into training- and testset for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n_train = int(n/10)\n",
    "\n",
    "train_x_1,test_x_1 = splitList(x_1,n_train)\n",
    "train_x_2,test_x_2 = splitList(x_2,n_train)\n",
    "train_labels,test_labels = splitList(labels,n_train)\n",
    "test_weights = splitList(weights,n_train)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Comparison, we calculate the best possible AMS    \n",
    "(case: every signal correctly detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def calcMaxAMS(weights,labels):\n",
    "    s,b = calcWeightSums(weights,labels,labels)\n",
    "    ams = calcAMS(s,b)\n",
    "    print(\"Maximum AMS possible with this Data:\", ams)\n",
    "    return ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "calcMaxAMS(test_weights,test_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize the Logistic Regression Classifier, shape the input-data and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "logReg = linMod.LogisticRegression(C=1e5)\n",
    "\n",
    "train_x = np.array([train_x_1,train_x_2]).transpose()\n",
    "test_x = np.array([test_x_1,test_x_2]).transpose()\n",
    "train_labels = np.array(train_labels).transpose()\n",
    "test_labels = np.array(test_labels).transpose()\n",
    "\n",
    "logReg.fit(train_x,train_labels)\n",
    "\n",
    "logReg.sparsify()\n",
    "\n",
    "predProb = logReg.predict_proba(test_x)\n",
    "pred = logReg.predict(test_x)\n",
    "score = logReg.score(test_x,test_labels)\n",
    "\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "s,b = calcWeightSums(test_weights,pred,test_labels)\n",
    "calcAMS(s,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully tested logistic Regression, now let's use it on actual CERN-Data.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import KaggleData;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "csvDict,header = KaggleData.createCsvDictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainingset has key \"t\"                   \n",
    "Public Testset has key \"p\" (note: \"p\" won't work, using private Testset (\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getFeatureSets(featureName):\n",
    "    trainFeature = KaggleData.getFeatureAsNpArray(\n",
    "        csvDict,header,featureName,[\"t\"],hasErrorValues = True)\n",
    "    testFeature = KaggleData.getFeatureAsNpArray(\n",
    "        csvDict,header,featureName,[\"v\"],hasErrorValues = True)\n",
    "    return trainFeature, testFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_eventList,test_eventList = getFeatureSets(\"EventId\")\n",
    "train_labels,test_labels = getFeatureSets(\"Label\")\n",
    "test_weights = getFeatureSets(\"KaggleWeight\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the relation Label <=> Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "signal_sum = int(test_labels.cumsum()[-1])\n",
    "background_sum = int(len(test_labels)-signal_sum)\n",
    "signal_weight = 0\n",
    "background_weight = 0\n",
    "for i in range(0,len(test_labels)):\n",
    "    if test_labels[i] > 0:\n",
    "        signal_weight += test_weights[i]\n",
    "    else:\n",
    "        background_weight += test_weights[i]\n",
    "print(background_weight/background_sum)\n",
    "print(signal_weight/signal_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe, that False signals will be weighted a lot heavier than True signals. \n",
    "\n",
    "If a classifier achieved a higher AMS while detecting less signals,   \n",
    "we can make statements about the usabilty of the features, the classifier used.\n",
    "\n",
    "We choose features with beneficial properties for classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "(train_DER_met_phi_centrality,\n",
    " test_DER_met_phi_centrality) = getFeatureSets(\"DER_met_phi_centrality\")\n",
    "(train_DER_pt_ratio_lep_tau,\n",
    " test_DER_pt_ratio_lep_tau) = getFeatureSets(\"DER_pt_ratio_lep_tau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DER_mass_MMC was not allowed in the former contest, we use it here anyway to test our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "(train_DER_mass_MMC,\n",
    " test_DER_mass_MMC) = getFeatureSets(\"DER_mass_MMC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_labels = np.array(train_labels).transpose()\n",
    "test_labels = np.array(test_labels).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "calcMaxAMS(test_weights,test_labels)\n",
    "print(\"True Signals:\",int(test_labels.cumsum()[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with one feature and add more with every regression to see improvement of the AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def logisticReg(train_x,train_labels,test_x,test_labels):\n",
    "    logReg = None\n",
    "    logReg = linMod.LogisticRegression(C=1e5)\n",
    "    logReg.fit(train_x,train_labels)\n",
    "    logReg.sparsify()\n",
    "    predProb = logReg.predict_proba(test_x)\n",
    "    pred = logReg.predict(test_x)\n",
    "    signals = int(pred.cumsum()[-1])  \n",
    "    print(\"signals read:\", signals)\n",
    "    if signals is not 0:\n",
    "        s,b = calcWeightSums(test_weights,pred,test_labels)\n",
    "        ams = calcAMS(s,b)\n",
    "    else:\n",
    "        ams = 0\n",
    "    print(\"AMS:\",ams)\n",
    "    return predProb,pred,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = np.array(\n",
    "    [train_DER_met_phi_centrality,\n",
    "     train_DER_pt_ratio_lep_tau]).transpose()\n",
    "test_x = np.array(\n",
    "    [test_DER_met_phi_centrality,\n",
    "     test_DER_pt_ratio_lep_tau]).transpose()\n",
    "pred = logisticReg(\n",
    "    train_x,train_labels,\n",
    "    test_x,test_labels)[1];\n",
    "pred.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def logRegFor(fList):\n",
    "    for feature in fList:\n",
    "        print(\"Feature:\",feature)\n",
    "        trainList_x,testList_x = getFeatureSets(feature)\n",
    "        train_x = np.array([trainList_x]).transpose()\n",
    "        test_x = np.array([testList_x]).transpose()\n",
    "        logisticReg(train_x,train_labels,test_x,test_labels)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "(train_PRI_tau_pt,\n",
    " test_PRI_tau_pt) = getFeatureSets(\"PRI_tau_pt\")\n",
    "(train_DER_met_phi_centrality,\n",
    " test_DER_met_phi_centrality) = getFeatureSets(\"DER_met_phi_centrality\")\n",
    "(train_DER_pt_h,\n",
    " test_DER_pt_h) = getFeatureSets(\"DER_pt_h\")\n",
    "(train_DER_pt_ratio_lep_tau,\n",
    " test_DER_pt_ratio_lep_tau) = getFeatureSets(\"DER_pt_ratio_lep_tau\")\n",
    "(train_DER_mass_transverse_met_lep,\n",
    " test_DER_mass_transverse_met_lep) = getFeatureSets(\"DER_mass_transverse_met_lep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are able to achieve a higher AMS by adjusting the decision-threshold (around 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def bestThreshold(predProb):\n",
    "    thresh = 0\n",
    "    maxAMS = 0\n",
    "    maxThresh = 0\n",
    "    for thresh in np.linspace(0.2,1.0,100):\n",
    "        newPred = np.zeros(len(predProb))\n",
    "        for i in range(0,len(predProb)):\n",
    "            if predProb[i][1] > thresh:\n",
    "                newPred[i]=1\n",
    "        s,b = calcWeightSums(test_weights,newPred,test_labels)\n",
    "        ams = calcAMS(s,b)\n",
    "        if ams > maxAMS:\n",
    "            maxThresh = thresh\n",
    "            maxAMS = ams\n",
    "            signals = int(newPred.cumsum()[-1])\n",
    "    print(\"Maximum AMS:\",maxAMS, \"with threshold\", maxThresh)\n",
    "    print(\"Signals read:\", signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = np.array(\n",
    "    [train_PRI_tau_pt,\n",
    "     train_DER_met_phi_centrality,\n",
    "     train_DER_pt_h,\n",
    "     train_DER_pt_ratio_lep_tau]).transpose()\n",
    "test_x = np.array(\n",
    "    [test_PRI_tau_pt,\n",
    "     test_DER_met_phi_centrality,\n",
    "     test_DER_pt_h,\n",
    "     test_DER_pt_ratio_lep_tau]).transpose()\n",
    "(predProb,\n",
    " pred) = logisticReg(\n",
    "    train_x,\n",
    "    train_labels,\n",
    "    test_x,\n",
    "    test_labels)[0:2];\n",
    "bestThreshold(predProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = np.array(\n",
    "    [train_PRI_tau_pt,\n",
    "     train_DER_met_phi_centrality]).transpose()\n",
    "test_x = np.array(\n",
    "    [test_PRI_tau_pt,\n",
    "     test_DER_met_phi_centrality]).transpose()\n",
    "predProb,pred = logisticReg(\n",
    "    train_x,\n",
    "    train_labels,\n",
    "    test_x,\n",
    "    test_labels)[0:2];\n",
    "bestThreshold(predProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = np.array(\n",
    "    [train_DER_met_phi_centrality,\n",
    "     train_DER_pt_ratio_lep_tau]).transpose()\n",
    "test_x = np.array(\n",
    "    [test_DER_met_phi_centrality,\n",
    "     test_DER_pt_ratio_lep_tau]).transpose()\n",
    "predProb,pred = logisticReg(\n",
    "    train_x,train_labels,\n",
    "    test_x,\n",
    "    test_labels)[0:2];\n",
    "bestThreshold(predProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x = np.array(\n",
    "    [train_DER_met_phi_centrality,\n",
    "     train_PRI_tau_pt]).transpose()\n",
    "test_x = np.array(\n",
    "    [test_DER_met_phi_centrality,\n",
    "     test_PRI_tau_pt]).transpose()\n",
    "predProb,pred = logisticReg(\n",
    "    train_x,\n",
    "    train_labels,\n",
    "    test_x,test_labels)[0:2];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
